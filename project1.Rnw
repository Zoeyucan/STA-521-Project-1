\documentclass[11pt]{article}
\usepackage{amsmath,amssymb,float,amsfonts,enumitem,tcolorbox}

\include{macros}

<<{r  global_options},echo=FALSE>>=
knitr::opts_chunk$set(include=TRUE,echo = TRUE, message = FALSE, warning = FALSE,
                      fig.width=3, fig.height=3, fig.align='center',
                      fig.asp = 1
                      )
@

\title{STA 521: Project 1 Redwood Data}
\author{Xinyi Sheng(xs110) Yucan Zeng(yc808)}
\date{}

\begin{document}
\SweaveOpts{concordance=TRUE}


\maketitle

\section*{Submission instructions}
It is a good idea to revisit your notes, slides and reading;
and synthesize their main points BEFORE doing the
project.

A main report (font size at least 11 pt, less or equal to 12 pages) generated by Latex or Rnw is required. 
The main report should address the questions below clearly and preferrably with figures. The clarity of your writing is also one important grading factor.  
Arrange the figures compactly if you use .Rnw to generate the report. Keep only the essential plots for the main report. You should aim your writing as smooth as a top research paper. Unlike your other homework, NO CODE should appear in the write-up. 
A .Rnw file corresponding to the project is also uploaded for you. You may use that to write up your solutions.

You may use the report title ``Project 1 Redwood Data Report'' or you can also be creative.
Put your name (with Student ID) and your teammate's name (with Student ID) in the \textbf{author} line below the title of your report.

The recommended work of this project is at least 16 hours (at least 8 hours / person). Note that it is almost impossible to finish the project in one day. Plan ahead and start early. 
\newline


You need to submit the following:
\begin{enumerate}
\item A pdf of your write-up ($\leq 12$ pages) to ``PROJ1 write-up''. NO Code. Please take care of your writing and figures. $20\%$ of total points will be removed for reports with more than 12 pages.
\item A .tex .R, .Rmd and/or .Rnw file, that has all your code, to ``PROJ1 code''.
\end{enumerate}
\emph{Ensure a proper submission to gradescope, otherwise it will not be graded. }

\newpage

Please read the submission guidelines properly to avoid confusions. Be aware that some of the questions are inherently open ended. Your answers will be graded based on not only the relevance, but also the clarity.

This project allows you to apply previously learned knowledge on data cleaning and data exploration on a real dataset. Here, we focus on data understanding and exploration using appropriate statistical methods and providing well explained visualization of the data, which might be useful for further study. Our work could be considered as an extension of the original paper (Tolle et al.) with statistics and visualization focus.

\section{Data collection (20 pts)}
The data is taken from Tolle et al.. A pdf of the paper can be found on together with problem statement on Sakai Resources. You should read this paper before doing the lab and understand the source of the data.
The main data files are packed in \textbf{redwooddata.tar.gz}. Take a look at the textfile "read-me" before touching the data. The main data files of interest in this project are \textbf{sonoma-data-all.csv} and \textbf{mote-location-data.txt}.
Explain to your teammate the main conclusion of the paper and how the sensors in the paper work (no need to write for this line). 

\begin{enumerate}[label=(\alph*)]
\item Write a summary (1/2 page) about the paper. At least, points such as the purpose of the study, where the data is collected,the main conclusion and impact should be covered.
\item Write a summary (1/2 page - 1 page) about the data collection. At least the following points should be covered: How are the sensors deployed? What is the duration of the data recording? What are the main variables of interest? What is the difference between the data in \textbf{sonoma-data-log.csv} and that in \textbf{sonoma-data-net.csv}.
\end{enumerate}

 <<>>=
library(dplyr)
library(ggplot2)
library(tidyverse)
require(tibble)
library(hrbrthemes)
library(scales)
library(units)
library(hablar)
@

<<>>=
#read data
df_all = read.csv("data/sonoma-data-all.csv",header=TRUE,stringsAsFactors=FALSE)
df_loc = read.csv('data/mote-location-data.txt',header=TRUE,stringsAsFactors=FALSE,sep='')
# df_all$result_date = format(as.POSIXct(df_all$result_time), format="%Y-%m-%d")
# df_all$result_second = format(as.POSIXct(df_all$result_time), format="%H:%M:%S")
df_net = read.csv("data/sonoma-data-net.csv",header=TRUE,stringsAsFactors=FALSE)
df_log = read.csv("data/sonoma-data-log.csv",header=TRUE,stringsAsFactors=FALSE)


#change dataframe to tibble
df_all = tibble(df_all)
df_loc = tibble(df_loc)
df_net = tibble(df_net)
df_log = tibble(df_log)

#add a new column to distinguish between net and log
df_net = df_net%>%mutate(type = 'net')
df_log = df_log%>%mutate(type = 'log')

#correct log time
df_net = df_net%>%mutate(actual_time= result_time)
df_log = df_log%>%mutate(actual_time= as.POSIXct("2004-04-27 17:10:00")+epoch*300)

#extract date
df_net$actual_date = as.Date(format(as.POSIXct(df_net$actual_time), format="%Y-%m-%d"))
df_log$actual_date = as.Date(format(as.POSIXct(df_log$actual_time), format="%Y-%m-%d"))

#merge df_all and df_loc
df_all_loc = df_all%>%left_join(df_loc,by = c("nodeid"="ID"))

#merge df_log and df_net
df_net_log = rbind(df_log, df_net)

#Drop NAs
df_net_log_NA = df_net_log%>%na.omit()

#Process the data using the data range given in the article(40w->25w)
##voltage in [2.4,3]
##humidity in [0, 100]
df_net_log_NA_filter = df_net_log_NA%>%filter(voltage<3,voltage>2.4,humidity>16.4,humidity<102)
df_log_filter = df_log%>%filter(humidity>16.4,humidity<100.2)
#use inner join to filter outlier nodeid(41w->40w)
df_net_log_NA_filter_loc = df_net_log_NA_filter%>%inner_join(df_loc,by = c("nodeid"="ID"))

@

<<>>=
#histogram
#Use origin data
#1)overall review
# Represent it
p <- df_net_log %>%
  ggplot(aes(x=actual_date, fill=type)) +
    geom_bar( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080")) +
    theme_ipsum() +
    labs(fill="") +
# Format : Week
scale_x_date(labels = date_format("%m/%d"),breaks = '5 days') +
  theme(axis.text.x = element_text(angle=45))+
  labs(title = "Data Distribution over Time by Log and Net")
p
@

<<>>=
#histogram
#2)voltage
p2 <- df_net_log %>%filter((type == 'log' & voltage<3 & voltage>2.4))%>%
  ggplot(aes(x=voltage, fill=type)) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2")) +
    theme_ipsum() +
    labs(fill="")
p2
p2_ <- df_net_log %>%filter((type == 'net' & voltage<250 & voltage>200))%>%
  ggplot(aes(x=voltage, fill=type)) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c( "#404080")) +
    theme_ipsum() +
    labs(fill="")
p2_
@

<<>>=
#histogram
#3)humidity
p3 <- df_net_log %>%filter(humidity>16.4 & humidity<100.2)%>%
  ggplot(aes(x=humidity, fill=type)) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity',binwidth = 5) +
    scale_fill_manual(values=c("#69b3a2", "#404080")) +
    theme_ipsum() +
    labs(fill="")
p3
@

<<>>=
#histogram
#4)temperature
p4 <- df_net_log %>%filter(humid_temp>6.6&humid_temp<32.6)%>%
  ggplot(aes(x=humid_temp, fill=type)) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity') +
    scale_fill_manual(values=c("#69b3a2", "#404080")) +
    theme_ipsum() +
    labs(fill="")
p4
@

<<>>=
#histogram
#5)hamatop
p5 <- df_net_log %>%mutate(hamatop_ppfd=0.0185*hamatop)%>%filter(hamatop_ppfd>=0&hamatop_ppfd<=2154)%>%
  ggplot(aes(x=hamatop_ppfd, fill=type)) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity',bins = 10) +
    scale_fill_manual(values=c("#69b3a2", "#404080")) +
    theme_ipsum() +
    labs(fill="")
p5
@


<<>>=
#histogram
#5)hamabot
p6 <- df_net_log %>%mutate(hamabot_ppfd=0.0185*hamabot)%>%filter(hamabot_ppfd>=0&hamabot_ppfd<=180)%>%
  ggplot(aes(x=hamabot_ppfd, fill=type)) +
    geom_histogram( color="#e9ecef", alpha=0.6, position = 'identity',bins = 10) +
    scale_fill_manual(values=c("#69b3a2", "#404080")) +
    theme_ipsum() +
    labs(fill="")
p6
@



\subsection{(2)}

<<>>=
#read data
df_net = read.csv("data/sonoma-data-net.csv",header=TRUE,stringsAsFactors=FALSE)
df_log = read.csv("data/sonoma-data-log.csv",header=TRUE,stringsAsFactors=FALSE)
df_loc = read.csv('data/mote-location-data.txt',header=TRUE,stringsAsFactors=FALSE,sep='')

#concert to tibble
df_net = tibble(df_net)
df_log = tibble(df_log)

#add a new column to distinguish between net and log
df_net = df_net%>%mutate(type = 'net')
df_log = df_log%>%mutate(type = 'log')

#correct log time
df_net = df_net%>%mutate(actual_time= result_time)
df_log = df_log%>%mutate(actual_time= as.POSIXct("2004-04-27 17:10:00")+epoch*300)

#extract date
df_net$actual_date = as.Date(format(as.POSIXct(df_net$actual_time), format="%Y-%m-%d"))
df_log$actual_date = as.Date(format(as.POSIXct(df_log$actual_time), format="%Y-%m-%d"))

#merge df_log and df_net
df_net_log = rbind(df_log, df_net)

#Drop NAs
df_net_log_NA = df_net_log%>%na.omit()

#remove duplicate
df_net_log_NA_dup = 
  df_net_log_NA[!duplicated(df_net_log_NA[,c(2,3,12)]),]

#Handle voltage outliers
df_net_log_NA_dup_volt = df_net_log_NA_dup%>%
  mutate(voltage_new = ifelse(type == 'net', voltage/83, voltage))

#Process the data using the data range given in the article(40w->25w)
##voltage in [2.4,3]
##humidity in [16.4, 100.2]
#ppfd of hamatop in[0,2154]
#ppfd of hamabot in[0,180]
df_net_log_NA_dup_volt_filter = df_net_log_NA_dup_volt%>%filter(
                                                voltage_new<3,voltage_new>2.4,
                                              humidity>16.4,humidity<100.2,
                                              humid_temp>6.6,humid_temp<32.6,
                                              0.0185*hamatop<2154,hamatop>=0,
                                              0.0185*hamabot<180,hamabot>=0)

#use inner join to filter outlier nodeid(41w->40w)
df_net_log_NA_dup_volt_filter_loc = df_net_log_NA_dup_volt_filter%>%inner_join(df_loc,by = c("nodeid"="ID"))
@

<<>>=
#boxplot
#huimidity, temperture, hamatop, hamabot

# loading data set and storing it in ds variable

df_net_log_NA_dup_volt_filter_loc%>%ggplot(aes(x = type, y = humidity, fill = type)) +
geom_boxplot(outlier.colour="black", outlier.shape=16, outlier.size=2)+scale_fill_brewer(palette="Dark1")+labs(title = 'boxplot of humidity')

@

<<>>=
df_net_log_NA_dup_volt_filter_loc%>%ggplot(aes(x = type, y = humid_temp, fill = type)) +
geom_boxplot(outlier.colour="black", outlier.shape=16, outlier.size=2)+scale_fill_brewer(palette="Dark1")+labs(title = 'boxplot of temperature')
@

<<>>=
df_net_log_NA_dup_volt_filter_loc%>%ggplot(aes(x = type, y = 0.0185*hamatop, fill = type)) +
geom_boxplot(outlier.colour="black", outlier.shape=16, outlier.size=2)+scale_fill_brewer(palette="Dark1")+labs(title = 'boxplot of hamatop_ppfd')
@

<<>>=
df_net_log_NA_dup_volt_filter_loc%>%ggplot(aes(x = type, y = 0.0185*hamabot, fill = type)) +
geom_boxplot(outlier.colour="black", outlier.shape=16, outlier.size=2)+scale_fill_brewer(palette="Dark1")+labs(title = 'boxplot of hamabot_ppfd')
@

<<>>=
#remove outliers
df_net_log_NA_dup_volt_filter_loc %>%
  group_by(type) %>%
  summarise(quantile(humidity,0.02)
)

df_net_log_NA_dup_volt_filter_loc %>%
  group_by(type) %>%
  summarise(quantile(humid_temp,0.98)
)

df_net_log_NA_dup_volt_filter_loc %>%
  group_by(type) %>%
  summarise(quantile(0.0185*hamatop,0.98)
)

df_net_log_NA_dup_volt_filter_loc %>%
  group_by(type) %>%
  summarise(quantile(0.0185*hamabot,0.98)
)
@

<<>>=
#filter 
#net humidity>53.1
#log temperature<25.7, net temperature<28.3
#log hamatop_ppfd<1912, net hamatop_ppfd<1897
#log hamabot_ppfd<60.2, net hamabot_ppfd<82.9
df_out = df_net_log_NA_dup_volt_filter_loc%>%filter(humidity>53.1&type == 'net'|type == 'log')%>%filter(humid_temp<25.7&type == 'log'|type == 'net')%>%filter(humid_temp<28.3&type == 'net'|type == 'log')%>%filter(0.0185*hamatop<1912&type == 'log'|type == 'net')%>%filter(0.0185*hamatop<1897&type == 'net'|type == 'log')%>%filter(0.0185*hamabot<60.2&type == 'log'|type == 'net')%>%filter(0.0185*hamabot<82.9&type == 'net'|type == 'log')

@



\subsection{3}
<<>>=
df_out$hour = format(as.POSIXct(df_out$actual_time), format="%H")
df_out_3_1 = df_out %>%
  filter(hour<=18&hour>=10)
df_out_3_2 = df_out %>%
  filter(as.numeric(hour)<=9&as.numeric(hour)>=6)
df_out_3_3 = df_out %>%
  filter(as.numeric(hour)<=20&as.numeric(hour)>=17)
@


<<>>=
#hour<=18&hour>=10
ggplot(data=df_out_3_1, aes(x=humidity, y=humid_temp,color = 'red'))+geom_point(size=3,alpha = 0.5)+geom_smooth(formula = y ~ x, method = "lm",col = 'black')
@


<<>>=
#as.numeric(hour)<=9&as.numeric(hour)>=6
ggplot(data=df_out_3_2, aes(x=humidity, y=humid_temp,color = 'red'))+geom_point(size=3,alpha = 0.5)+geom_smooth(formula = y ~log(x), method = "lm",col = 'black')
@


<<>>=
#as.numeric(hour)<=20&as.numeric(hour)>=17
ggplot(data=df_out_3_3, aes(x=humidity, y=humid_temp,color = 'red'))+geom_point(size=3,alpha = 0.5)+geom_smooth(formula = y ~ x, method = "lm",col = 'black')
@


\subsection{(2)}


\section{Data cleaning (40 pts)}
This data set is quite raw - it contains some gross outliers, inconsistencies, and lots of missing values. Read the \textbf{Outlier rejection} section in the paper carefully and critically. You don't have to bindly follow their data cleaning method.

The file \textbf{sonoma-data-all.csv} is a simple concatenation of the two files  \textbf{sonoma-data-log.csv} and \textbf{sonoma-data-net.csv}. However, doing the merge of two data files requires that they are consistent. nodeid and epoch together provides a unique identifier for one measure. But some other variables are not consistent.  
\begin{enumerate}[label=(\alph*)]
\item Check histograms of each variable in two data files (Plot only the ones that you think are interesting or relevant). Which variable is not consistent? Convert the data to the same range. NO CODE but explain clearly what you did. 
\item Remove missing data. Comment on the number of missing measurements and the corresponding date and time period.
\item The location data is separate in another file \textbf{mote-location-data.txt}. Incorporate it in the main table. Hint: here the nodeid serves a key to add columns for height, direction, distance and tree. State the number of variables in your new data frame.
\item Use histogram and quantiles to visually identify easy outliers for each of the four variables: humidity, humid temp, hamatop, hamabot. And remove them. Comment on the rationality behind your removal.
\item (Bonus) Discuss other possible outliers and explain your reason why it is better to remove them than to keep them.
\end{enumerate}



\section{Data Exploration (40 pts)}
\begin{enumerate}[label=(\alph*)]
\item Make some pairwise scatterplots of some variables. Pick a reasonable time period. Explain your choice and describe your findings.
\item Are any of the predictors associated with Incident PAR? If so, explain the relationship.
\item Each variable of our data basically have three dimensions: value, height and time. Consider each variable as a time series and look at its temporal trend. Generate such plots (value vs time) with height as color cue for at least four variables (Temperature, Relative Humidity, Incident PAR and Reflected PAR). You can do it for different time scales (during an hour, during a day or during the entire experiment). However, at least the plots with days as x-axis are required. Comment on the range, continuity and strange behaviors in these variables.
\item After PCA analysis, generate scree plot of the data. Can this data be approximated by some low-dimensional representation? 
\end{enumerate}

\section{Interesting Findings (15 * 2 pts)}
Describe two/three interesting findings from exploratory analysis of the data. Try to use the techniques that you have learned, such as histograms, PCA, K-means, GMM and hierachical clustering etc. Note that even though you got a dataframe with only a few columns, you may reshape the dataframe before doing any EDA, such as reorganizing such that aggregated information in each day is a column, or a particular hour in each day is a column. Comment on your interesting findings. Different bonuses are given based on how interesing your result is. 
\begin{enumerate}[label=(\alph*)]
\item Finding 1.
\item Finding 2.
\item (Bonus) Finding 3. Bonus is given only if we find all three findings interesting. 
\end{enumerate}

\section{Graph Critique in the paper (40 pts)}
The overall quality of the paper by Tolle et al. is good. However, some plots are not perfect from a statistician's point of view.
\begin{enumerate}[label=(\alph*)]
\item Figure 3[a] shows the distributions of sensor readings projected onto the value dimension, using a histogram. It turns out that both the incident and reflected PAR have long tail. We could not read full information from this histogram. Try to make a better plot with log transform of the data. 
\item What message do the boxplots in Figure 3[c] and 3[d] try to convey? Do you think the plots convey the right messages? If not generate a new plot with the same data. Hint: compare to some plots in Figure 4.
\item Any suggestions for improving the first two plots in Figure 4? Can you distinguish all the colors in these two plots?
\item Comment on Figure 7. Is it possible to generate a better visualization to highlight the difference between network and log data?
\end{enumerate}

\end{document}